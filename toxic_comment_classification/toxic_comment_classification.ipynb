{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Transformer Classifier\n",
    "### with HuggingFace and Tensorflow 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slient install\n",
    "!pip install -q -r requirements.txt\n",
    "!pip install  -q -i https://test.pypi.org/simple/ EuroPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.3.1\n",
      "GPU Devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# all imports\n",
    "import os, time\n",
    "from datetime import datetime\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "import europy\n",
    "from europy.helpers import load_global_params\n",
    "from europy.decorators import using_params\n",
    "from europy.decorators import model_details\n",
    "from europy.lifecycle import reporting\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "print(f'TensorFlow Version: {tf.__version__}')\n",
    "print(f'GPU Devices: {tf.config.list_physical_devices(\"GPU\")}')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import TFBertModel\n",
    "from transformers import create_optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = load_global_params('params.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/b/.kaggle/kaggle.json'\n",
      "Downloading jigsaw-toxic-comment-classification-challenge.zip to /home/b/dev/EuroPy-Examples/toxic_comment_classification\n",
      " 97%|████████████████████████████████████▊ | 51.0M/52.6M [00:02<00:00, 29.4MB/s]\n",
      "100%|██████████████████████████████████████| 52.6M/52.6M [00:02<00:00, 25.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c jigsaw-toxic-comment-classification-challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n",
      "Archive:  jigsaw-toxic-comment-classification-challenge.zip\n",
      "  inflating: data/sample_submission.csv.zip  \n",
      "  inflating: data/test.csv.zip       \n",
      "  inflating: data/test_labels.csv.zip  \n",
      "  inflating: data/train.csv.zip      \n",
      "Archive:  data/sample_submission.csv.zip\n",
      "  inflating: data/sample_submission.csv  \n",
      "Archive:  data/train.csv.zip\n",
      "  inflating: data/train.csv          \n",
      "Archive:  data/test.csv.zip\n",
      "  inflating: data/test.csv           \n",
      "Archive:  data/test_labels.csv.zip\n",
      "  inflating: data/test_labels.csv    \n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!unzip -o jigsaw-toxic-comment-classification-challenge.zip -d data/\n",
    "!rm jigsaw-toxic-comment-classification-challenge.zip\n",
    "!unzip -o data/sample_submission.csv.zip -d data/\n",
    "!rm data/sample_submission.csv.zip\n",
    "!unzip -o data/train.csv.zip -d data/\n",
    "!rm data/train.csv.zip\n",
    "!unzip -o data/test.csv.zip -d data/\n",
    "!rm data/test.csv.zip\n",
    "!unzip -o data/test_labels.csv.zip -d data/\n",
    "!rm data/test_labels.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'data/train.csv'\n",
    "test_path = 'data/test.csv'\n",
    "test_labels_path = 'data/test_labels.csv'\n",
    "subm_path = 'data/sample_submission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "df_test_labels = pd.read_csv(test_labels_path).set_index('id')\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DistilBERT Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    params['pre_trained_model'],\n",
    "    do_lower_case=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@using_params('params.yml')\n",
    "def tokenize_sentences(sentences, tokenizer, max_seq_len=128):\n",
    "    tokenized_sentences = []\n",
    "    \n",
    "    for sentence in tqdm(sentences):\n",
    "        tokenized_sentence = tokenizer.encode(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_seq_len\n",
    "        )\n",
    "        tokenized_sentences.append(tokenized_sentence)\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attention_masks(tokenized_add_padded_sentences):\n",
    "    attention_masks = []\n",
    "    \n",
    "    for sentence in tqdm(tokenized_add_padded_sentences):\n",
    "        att_mask = [int(token_id > 0) for token_id in sentence]\n",
    "        attention_masks.append(att_mask)\n",
    "    return np.asarray(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/159571 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|██████████| 159571/159571 [02:22<00:00, 1121.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# create tokenized sentences (will take a few minutes)\n",
    "input_ids = tokenize_sentences(\n",
    "    df_train['comment_text'],\n",
    "    tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = pad_sequences(\n",
    "    input_ids,\n",
    "    maxlen=params['max_seq_len'],\n",
    "    dtype='long',\n",
    "    value=0,\n",
    "    truncating='post',\n",
    "    padding='post'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159571/159571 [00:07<00:00, 21584.30it/s]\n"
     ]
    }
   ],
   "source": [
    "attention_masks = create_attention_masks(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "labels = df_train[params['label_cols']].values\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n",
    "    input_ids,\n",
    "    labels,\n",
    "    random_state=0,\n",
    "    test_size=params['test_size']\n",
    ")\n",
    "train_masks, validation_masks, _, _ = train_test_split(\n",
    "    attention_masks,\n",
    "    labels,\n",
    "    random_state=0,\n",
    "    test_size=params['test_size']\n",
    ")\n",
    "\n",
    "train_size = len(train_inputs)\n",
    "validation_size = len(validation_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@using_params('params.yml')\n",
    "def create_dataset(data_tuple, num_epochs=1, batch_size=32, buffer_size=10000, train=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data_tuple)\n",
    "    if train:\n",
    "        dataset = dataset.shuffle(buffer_size=buffer_size)\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    if train:\n",
    "        dataset.prefetch(1)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train & validation datasets\n",
    "train_dataset = create_dataset(\n",
    "    (train_inputs, train_masks, train_labels)\n",
    ")\n",
    "validation_dataset = create_dataset(\n",
    "    (validation_inputs, validation_masks, validation_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "class BertClassifier(tf.keras.Model):\n",
    "    def __init__(self, bert: TFBertModel, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.classifier = Dense(num_classes, activation='sigmoid')\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask\n",
    "        )\n",
    "        \n",
    "        cls_output = outputs[1]\n",
    "        cls_output = self.classifier(cls_output)\n",
    "        \n",
    "        return cls_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# init the model\n",
    "model = BertClassifier(TFBertModel.from_pretrained(params['pre_trained_model']), len(params['label_cols']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optimizer\n",
    "steps_per_epoch = train_size // params['batch_size']\n",
    "validation_steps = validation_size // params['batch_size']\n",
    "\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "validation_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "\n",
    "warmup_steps = steps_per_epoch // 3\n",
    "total_steps = steps_per_epoch * params['num_epochs'] - warmup_steps\n",
    "optimizer, lr_scheduler = create_optimizer(\n",
    "    init_lr=params['learning_rate'],\n",
    "    num_train_steps=total_steps,\n",
    "    num_warmup_steps=warmup_steps\n",
    ")\n",
    "\n",
    "train_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(params['label_cols']))]\n",
    "validation_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(params['label_cols']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loops\n",
    "@tf.function\n",
    "def train_step(model, token_ids, masks, labels):\n",
    "    labels = tf.dtypes.cast(labels, tf.float32)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(token_ids, attention_mask=masks)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    print(optimizer)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    \n",
    "    for i, auc in enumerate(train_auc_metrics):\n",
    "        auc.update_state(labels[:,i], predictions[:,i])\n",
    "\n",
    "@tf.function\n",
    "def validation_step(model, token_ids, masks, labels):\n",
    "    labels = tf.dtypes.cast(labels, tf.float32)\n",
    "    \n",
    "    predictions = model(token_ids, attention_mask=masks, training=False)\n",
    "    v_loss = loss_object(labels, prediction)\n",
    "    \n",
    "    validation_loss(v_loss)\n",
    "    for i, auc in enumerate(validation_auc_metrics):\n",
    "        auc.update_state(labels[:,i], predictions[:,i])\n",
    "        \n",
    "\n",
    "@model_details('model_details.yml')\n",
    "@using_params('params.yml')\n",
    "def train(model, train_dataset, val_dataset, train_steps_per_epoch, val_steps_per_epoch, num_epochs=1, label_cols=[]):\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        for i, (token_ids, masks, labels) in enumerate(tqdm(train_dataset, total=train_steps_per_epoch)):\n",
    "            train_step(model, token_ids, masks, labels)\n",
    "            if i % 1000 == 0:\n",
    "                print(f'\\nTrain Step: {i}, Loss: {train_loss.result()}')\n",
    "                for i, label_name in enumerate(label_cols):\n",
    "                    print(f'{label_name} roc_auc {train_auc_metrics[i].result()}')\n",
    "                    train_auc_metrics[i].reset_states()\n",
    "                    \n",
    "        for i, (token_ids, masks, labels) in enumerate(tqdm(val_dataset, val_steps_per_epoch)):\n",
    "            validation_step(model, token_ids, masks, labels)\n",
    "            print(f'\\n{Epoch} {epoch+1}, Validation Loss: {validation_loss.result()}, Time: {time.time()-start}\\n')\n",
    "            for i, label_name in enumerate(label_cols):\n",
    "                print(f'{label_name} roc_auc {validation_auc_metrics[i].result()}')\n",
    "                validation_auc_metrics[i].reset_states()\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4487 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7fc598cb2040>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/4487 [00:03<3:55:32,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Step: 0, Loss: 1.0569530725479126\n",
      "toxic roc_auc 0.3895631730556488\n",
      "severe_toxic roc_auc 0.7484276294708252\n",
      "obscene roc_auc 0.6311532855033875\n",
      "threat roc_auc 0.2912317216396332\n",
      "insult roc_auc 0.5377174019813538\n",
      "identity_hate roc_auc 0.4254571199417114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 167/4487 [01:39<41:42,  1.73it/s]"
     ]
    }
   ],
   "source": [
    "# run the training loop (GPU Required)\n",
    "#  - ~45 minutes per epoch on GTC 1080 8gb GDDR5\n",
    "train(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    validation_dataset,\n",
    "    train_steps_per_epoch=steps_per_epoch,\n",
    "    val_steps_per_epoch=validation_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153164/153164 [02:10<00:00, 1172.72it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'max_len'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-d21e7a488523>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create testing tokens (will take a few minutes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'long'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncating\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_attention_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_attention_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'max_len'"
     ]
    }
   ],
   "source": [
    "# create testing tokens (will take a few minutes)\n",
    "test_input_ids = tokenize_sentences(df_test['comment_text'], tokenizer)\n",
    "test_input_ids = pad_sequences(test_input_ids, maxlen=params.max_len, dtype='long', value=0, truncating='post', padding='post')\n",
    "test_attention_masks = create_attention_masks(test_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run inference\n",
    "# - ~15 minutes on GTX 1080\n",
    "test_step = len(df_test) // params.batch_size\n",
    "print(test_step)\n",
    "\n",
    "test_dataset = create_dataset(\n",
    "    (test_input_ids, test_attention_masks),\n",
    "    batch_size=params.batch_size,\n",
    "    train=False, \n",
    "    epochs=1\n",
    ")\n",
    "\n",
    "df_submission = pd.read_csv('data/sample_submission.csv', index_col='id')\n",
    "\n",
    "for i, (token_ids, masks) in enumerate(tqdm(test_dataset, total=test_step)):\n",
    "    sample_ids = df_test.iloc[i*params.batch_size:(i+1)*params.batch_size]['id']\n",
    "    predictions = model(token_ids, attention_mask=masks).numpy()\n",
    "    \n",
    "    def_submission.loc[sample_ids, label_cols] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump results\n",
    "df_submission.to_csv('data/submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models\n",
    "now = datetime.now()\n",
    "model.save_weights(f'models/{now:%Y-%m-%d %H:%M}_bert.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Explore Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporting.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "europy",
   "language": "python",
   "name": "europy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slient install\n",
    "!pip install -q -r requirements.txt\n",
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/b/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import itertools\n",
    "import requests\n",
    "\n",
    "import europy\n",
    "from europy.notebook import load_global_params\n",
    "from europy.decorator import using_params, bias, data_bias, fairness, accountability, transparency, minimum_functionality, accuracy\n",
    "from europy.decorator import model_details\n",
    "from europy.lifecycle import reporting\n",
    "from europy.lifecycle.reporting import execute_tests, report_model_details, report_model_params, generate_report\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, roc_auc_score, auc, roc_curve, classification_report\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from model import BertClassifier\n",
    "from transformers import TFBertModel"
   ]
  },
  {
   "source": [
    "## Download Data\n",
    "(this is only needed once)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Download & Load Model\n",
    "Previously trained using `toxic_comment_classification.ipynb` --> results uploaded to s3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "415792766"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# download model data\n",
    "url = 'https://blainerothrock-public.s3.us-east-2.amazonaws.com/europy/toxic_comment_classification_v1.zip'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "open('data/toxic_comment_classification_v1.zip', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n",
      "Archive:  data/toxic_comment_classification_v1.zip\n",
      "   creating: data/toxic_comment_classification_v1/\n",
      "  inflating: data/toxic_comment_classification_v1/params.yml  \n",
      "  inflating: data/toxic_comment_classification_v1/submission.csv  \n",
      "  inflating: data/toxic_comment_classification_v1/model_details.yml  \n",
      "  inflating: data/toxic_comment_classification_v1/2020-11-23 08:42_bert.h5  \n",
      "  inflating: data/toxic_comment_classification_v1/model.py  \n",
      "  inflating: data/toxic_comment_classification_v1/obscene_words.txt  \n"
     ]
    }
   ],
   "source": [
    "# unzip model data\n",
    "!mkdir data\n",
    "!unzip data/toxic_comment_classification_v1.zip -d data/\n",
    "!rm data/toxic_comment_classification_v1.zip"
   ]
  },
  {
   "source": [
    "### Download Jigsaw Dataset\n",
    "From [Kaggle](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)\n",
    "Requires a Kaggle account and a auth token stored at `~/.kaggle/kaggle.json`. See [Kaggle's API documentation](https://github.com/Kaggle/kaggle-api#api-credentials) for more details."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/b/.kaggle/kaggle.json'\n",
      "jigsaw-toxic-comment-classification-challenge.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "# download kaggle dataset\n",
    "!kaggle competitions download -c jigsaw-toxic-comment-classification-challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Archive:  jigsaw-toxic-comment-classification-challenge.zip\n",
      "  inflating: data/sample_submission.csv.zip  \n",
      "  inflating: data/test.csv.zip       \n",
      "  inflating: data/test_labels.csv.zip  \n",
      "  inflating: data/train.csv.zip      \n",
      "Archive:  data/sample_submission.csv.zip\n",
      "  inflating: data/sample_submission.csv  \n",
      "Archive:  data/train.csv.zip\n",
      "  inflating: data/train.csv          \n",
      "Archive:  data/test.csv.zip\n",
      "  inflating: data/test.csv           \n",
      "Archive:  data/test_labels.csv.zip\n",
      "  inflating: data/test_labels.csv    \n"
     ]
    }
   ],
   "source": [
    "# unzip kaggle data and delete zip\n",
    "!unzip -o jigsaw-toxic-comment-classification-challenge.zip -d data/\n",
    "!rm jigsaw-toxic-comment-classification-challenge.zip\n",
    "!unzip -o data/sample_submission.csv.zip -d data/\n",
    "!rm data/sample_submission.csv.zip\n",
    "!unzip -o data/train.csv.zip -d data/\n",
    "!rm data/train.csv.zip\n",
    "!unzip -o data/test.csv.zip -d data/\n",
    "!rm data/test.csv.zip\n",
    "!unzip -o data/test_labels.csv.zip -d data/\n",
    "!rm data/test_labels.csv.zip"
   ]
  },
  {
   "source": [
    "## Set Up\n",
    "Start here if data is already downloaded"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "========= EuroPy Captured Params: (global) =========\n  - global.pre_trained_model: bert-base-uncased\n  - global.max_seq_len: 128\n  - global.train_percent: 0.1\n  - global.batch_size: 32\n  - global.num_epochs: 1\n  - global.label_cols: ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n  - global.test_size: 0.1\n  - global.learning_rate: 2e-05\n"
     ]
    }
   ],
   "source": [
    "# load global params from the model training\n",
    "params = load_global_params('data/toxic_comment_classification_v1/params.yml')\n",
    "model_path = 'data/toxic_comment_classification_v1/2020-11-23 08:42_bert.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create BERT tokenizer from Huggingface\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    params['pre_trained_model'],\n",
    "    do_lower_case=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "Model: \"bert_classifier\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "tf_bert_model (TFBertModel)  multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  4614      \n",
      "=================================================================\n",
      "Total params: 109,486,854\n",
      "Trainable params: 109,486,854\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model from downloaded weights\n",
    "\n",
    "def load_model(weight_path=\"data/models/2020-11-23 08:42_bert.h5\", params=None):\n",
    "    model = BertClassifier(TFBertModel.from_pretrained(params['pre_trained_model']), len(params['label_cols']))\n",
    "    encoded = tokenizer(\"init the model\", return_tensors='tf')\n",
    "    model(**encoded)\n",
    "    model.load_weights(weight_path)\n",
    "    return model\n",
    "\n",
    "model = load_model(model_path, params)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Jigsaw data and submission from model (testing prediction)\n",
    "\n",
    "submission_df = pd.read_csv('data/toxic_comment_classification_v1/submission.csv')\n",
    "test_labels = pd.read_csv('data/test_labels.csv')\n",
    "test_labels_gf = test_labels.copy()\n",
    "\n",
    "# rename labels to combine with testing predictions\n",
    "cols_rn = {}\n",
    "for label in params['label_cols']:\n",
    "    cols_rn[label] = label+'_gt'\n",
    "test_labels_gf.rename(columns=cols_rn, inplace=True)\n",
    "\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "\n",
    "obscene_words = open(\"data/toxic_comment_classification_v1/obscene_words.txt\", \"r\").readlines()\n",
    "obscene_words = [w.strip('\\n').lower() for w in obscene_words]\n",
    "\n",
    "\n",
    "# merge generated submission\n",
    "# (test test_df contains both prediction and ground truth labels)\n",
    "test_df = test_df.merge(submission_df, on='id').dropna()\n",
    "test_df = test_df.merge(test_labels_gf, on='id').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model report with model details and parameters from the downloaded model\n",
    "report_model_details('data/toxic_comment_classification_v1/model_details.yml')\n",
    "report_model_params('data/toxic_comment_classification_v1/params.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Data"
   ]
  },
  {
   "source": [
    "### Accurarcy Tests\n",
    "Check the Precision, Recall and Accurarcy of the model (assuming a 0.5 classification threshold)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@accuracy(\n",
    "    \"Classification Report\",\n",
    "    \"Test precision, recall, accurarcy, and f1-score of each class as if >0.5 is considered classified as a label\")\n",
    "def test_classificaiton_reprot(df, labels, pos=0.5):\n",
    "    results = []\n",
    "    for l in labels:\n",
    "        preds = np.array(list(map(lambda x: 1 if x > pos else 0, df[label].to_numpy())))\n",
    "        gt = np.array(list(map(lambda x: 1 if x == 1.0 else 0, df[l+'_gt'].to_numpy())))\n",
    "\n",
    "        result = classification_report(gt, preds, labels=[0,1], target_names=[f'not {l}', l], output_dict=True)\n",
    "\n",
    "        r = {'label': l}\n",
    "        r['accuracy'] = result['accuracy']\n",
    "        r['weighted_precision'] = result['weighted avg']['precision']\n",
    "        r['weighted_recall'] = result['weighted avg']['recall']\n",
    "        r['weighted_f1_score'] = result['weighted avg']['f1-score']\n",
    "        r['support_pos'] = result[l]['support']\n",
    "        r['f1_score_pos'] = result[l]['f1-score']\n",
    "        r['support_neg'] = result[f'not {l}']['support']\n",
    "        r['f1_score_neg'] = result[f'not {l}']['f1-score']\n",
    "\n",
    "        results.append(r)\n",
    "\n",
    "\n",
    "    return pd.DataFrame.from_dict(results)\n",
    "\n",
    "@accuracy(\n",
    "    \"ROC AUC\", \n",
    "    \"Area Under the Curve score for each classification (this was the training metric)\"\n",
    ")\n",
    "def calc_roc_auc(df, labels):\n",
    "    results = []\n",
    "    for l in labels:\n",
    "        preds = df[label].to_numpy()\n",
    "        gt = df[l+'_gt'].to_numpy()\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(gt, preds, pos_label=1)\n",
    "        auc_score = auc(fpr, tpr)\n",
    "        results.append({\n",
    "            'label': l,\n",
    "            'auc_score': auc_score\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame.from_dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Execute - Classification Report (['accuracy'])\n",
      "\tPASS\n",
      "Execute - ROC AUC (['accuracy'])\n",
      "\tPASS\n",
      "========= EuroPy Test Results =========\n",
      "Total Tests: 2\n",
      "Passing: 2\n",
      "Failing: 0\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                     key                                        description  \\\n",
       "0  Classification Report  Test precision, recall, accurarcy, and f1-scor...   \n",
       "1                ROC AUC  Area Under the Curve score for each classifica...   \n",
       "\n",
       "       labels                                             result figures  \\\n",
       "0  [accuracy]             label  accuracy  weighted_precision...      []   \n",
       "1  [accuracy]             label  auc_score\n",
       "0          toxic  ...      []   \n",
       "\n",
       "   success  \n",
       "0     True  \n",
       "1     True  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>key</th>\n      <th>description</th>\n      <th>labels</th>\n      <th>result</th>\n      <th>figures</th>\n      <th>success</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Classification Report</td>\n      <td>Test precision, recall, accurarcy, and f1-scor...</td>\n      <td>[accuracy]</td>\n      <td>label  accuracy  weighted_precision...</td>\n      <td>[]</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ROC AUC</td>\n      <td>Area Under the Curve score for each classifica...</td>\n      <td>[accuracy]</td>\n      <td>label  auc_score\n0          toxic  ...</td>\n      <td>[]</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "execute_tests(df=test_df, labels=params['label_cols'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "========= EuroPy Report Generated =========\nReport output: file:///home/b/.europy/reports/EuroPy_Test_Report_06122020_143942/report.md\n"
     ]
    }
   ],
   "source": [
    "generate_report(clear_report=False)"
   ]
  },
  {
   "source": [
    "### Transparency and data bias tests"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare common words from training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleep(w, obscene_words):\n",
    "    if w in obscene_words:\n",
    "        mask = len(w[1:-1]) * '*'\n",
    "        word_mask = w[0] + mask + w[-1]\n",
    "        return word_mask\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_freq(df, label, threshold=0.9):\n",
    "    result = {'label': label}\n",
    "    word_freq = {}\n",
    "    \n",
    "    sps = set(stopwords.words('english'))\n",
    "    \n",
    "    sub_df = df.where(df[label] > threshold).dropna()\n",
    "    \n",
    "    comment_words = sub_df.comment_text.to_numpy().flatten().tolist()\n",
    "    comment_words = \" \".join(comment_words).split(\" \")\n",
    "    comment_words = [w.lower() for w in comment_words]\n",
    "    comment_words = list(filter(lambda w: w.isalpha() and len(w) > 2 and w not in sps, comment_words))\n",
    "\n",
    "    for w in comment_words:\n",
    "        word_freq[w] = word_freq.get(w, 0) + 1\n",
    "\n",
    "    sorted_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True) \n",
    "\n",
    "    for i in range(10):\n",
    "        result[f'{i}_word'] = sorted_freq[i][0]\n",
    "        result[f'{i}_count'] = sorted_freq[i][1]\n",
    "\n",
    "    return result\n",
    "\n",
    "@transparency(\n",
    "    \"Top 10 Word Counts by Label for testing data\", \n",
    "    \"Comparision of word counts in testing data. Includes top 10 words\"\n",
    ")\n",
    "def test_data_word_count(df, labels, threshold=0.75):\n",
    "    results = []\n",
    "    for l in labels:\n",
    "        results.append(generate_word_freq(df, l, threshold=threshold))\n",
    "\n",
    "    return pd.DataFrame.from_dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Execute - Top 10 Word Counts by Label for testing data (['transparency'])\n",
      "\tPASS\n",
      "========= EuroPy Test Results =========\n",
      "Total Tests: 1\n",
      "Passing: 1\n",
      "Failing: 0\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                            key  \\\n",
       "0  Top 10 Word Counts by Label for testing data   \n",
       "\n",
       "                                         description          labels  \\\n",
       "0  Comparision of word counts in testing data. In...  [transparency]   \n",
       "\n",
       "                                              result figures  success  \n",
       "0             label  0_word  0_count  1_word  1_c...      []     True  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>key</th>\n      <th>description</th>\n      <th>labels</th>\n      <th>result</th>\n      <th>figures</th>\n      <th>success</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Top 10 Word Counts by Label for testing data</td>\n      <td>Comparision of word counts in testing data. In...</td>\n      <td>[transparency]</td>\n      <td>label  0_word  0_count  1_word  1_c...</td>\n      <td>[]</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "execute_tests(df=test_df, labels=params['label_cols'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transparency(\n",
    "    \"Top 10 Word Counts by Label for training data\", \n",
    "    \"Comparision of word counts in training data. Includes top 10 words\"\n",
    ")\n",
    "def test_data_word_count(df, labels, threshold=0.75):\n",
    "    results = []\n",
    "    for l in labels:\n",
    "        results.append(generate_word_freq(df, l, threshold=threshold))\n",
    "\n",
    "    return pd.DataFrame.from_dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Execute - Top 10 Word Counts by Label for training data (['transparency'])\n",
      "\tPASS\n",
      "========= EuroPy Test Results =========\n",
      "Total Tests: 1\n",
      "Passing: 1\n",
      "Failing: 0\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                             key  \\\n",
       "0  Top 10 Word Counts by Label for training data   \n",
       "\n",
       "                                         description          labels  \\\n",
       "0  Comparision of word counts in training data. I...  [transparency]   \n",
       "\n",
       "                                              result figures  success  \n",
       "0             label  0_word  0_count   1_word  1_...      []     True  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>key</th>\n      <th>description</th>\n      <th>labels</th>\n      <th>result</th>\n      <th>figures</th>\n      <th>success</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Top 10 Word Counts by Label for training data</td>\n      <td>Comparision of word counts in training data. I...</td>\n      <td>[transparency]</td>\n      <td>label  0_word  0_count   1_word  1_...</td>\n      <td>[]</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "execute_tests(df=train_df, labels=params['label_cols'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "========= EuroPy Report Generated =========\nReport output: file:///home/b/.europy/reports/EuroPy_Test_Report_06122020_143942/report.md\n"
     ]
    }
   ],
   "source": [
    "generate_report(clear_report=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Distrubutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_classes(df, threshold, label_cols):\n",
    "    counts = [0] * len(label_cols)\n",
    "    for i, label in enumerate(label_cols):\n",
    "        values = df[label].to_list()\n",
    "        for value in values:\n",
    "            if value > threshold:\n",
    "                counts[i] += 1\n",
    "    return counts\n",
    "    \n",
    "\n",
    "def class_dist(train_df, test_df, test_labels_df, label_cols, threshold=0.9, no_class=False, plot_name=\"Class_Distribution\", plots={}):    \n",
    "    train_counts = count_classes(train_df, threshold, label_cols)\n",
    "    test_pred_counts = count_classes(test_df, threshold, label_cols)\n",
    "    test_label_counts = count_classes(test_labels_df, threshold, label_cols)\n",
    "    \n",
    "    if no_class:\n",
    "        label_cols.append('None')\n",
    "        train_counts.append(train_df.shape[0] - sum(train_counts))\n",
    "        test_pred_counts.append(test_df.shape[0] - sum(test_pred_counts))\n",
    "        test_label_counts.append(test_labels_df.shape[0] - sum(test_label_counts))\n",
    "    \n",
    "    dist_data = []\n",
    "    for i, label in enumerate(label_cols):\n",
    "        dist = {'label': label}\n",
    "        dist['train_actual'] = train_counts[i]\n",
    "        dist['test_predictions'] = test_pred_counts[i]\n",
    "        dist['test_actuals'] = test_label_counts[i]\n",
    "\n",
    "    x = np.arange(len(label_cols))  # the label locations\n",
    "    width = 0.2\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    rects1 = ax.bar(x, train_counts, width, label='Train Actual')\n",
    "    rects2 = ax.bar(x + width, test_pred_counts, width, label='Test Predictions')\n",
    "    rects3 = ax.bar(x - width, test_label_counts, width, label='Test Actual')\n",
    "    \n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Counts by Classification')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(label_cols)\n",
    "    ax.legend()\n",
    "    \n",
    "    fig.tight_layout()\n",
    "   \n",
    "    plots[plot_name] = plt\n",
    "    \n",
    "    return pd.DataFrame.from_dict(dist_data)\n",
    "\n",
    "@data_bias(\n",
    "    \"Classification Counts by Training Actuals, Testing Actuals, and Training Predictions\",\n",
    "    \"Uses a threshold of 0.75 to determine classification\"\n",
    ")\n",
    "def class_dist_class_only(train_df, test_df, test_labels, label_cols, plots={}):\n",
    "    return class_dist(\n",
    "        train_df, \n",
    "        test_df, \n",
    "        test_labels, \n",
    "        label_cols, \n",
    "        threshold=0.75, \n",
    "        no_class=False, \n",
    "        plot_name=\"Class_Distribution_Class_Only\",\n",
    "        plots=plots\n",
    "    )\n",
    "\n",
    "@data_bias(\n",
    "    \"Classification Counts by Training Actuals, Testing Actuals, and Training Predictions -- Includes No Classification\",\n",
    "    \"Uses a threshold of 0.75 to determine classification\"\n",
    ")\n",
    "def class_dist_including_non_class(train_df, test_df, test_labels, label_cols, plots={}):\n",
    "    return class_dist(\n",
    "        train_df,\n",
    "        test_df,\n",
    "        test_labels,\n",
    "        label_cols,\n",
    "        threshold=0.75,\n",
    "        no_class=True, \n",
    "        plot_name=\"Class_Distribution_With_Non_Class\",\n",
    "        plots=plots\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Execute - Classification Counts by Training Actuals, Testing Actuals, and Training Predictions (['data-bias'])\n",
      "\tPASS\n",
      "Execute - Classification Counts by Training Actuals, Testing Actuals, and Training Predictions -- Includes No Classification (['data-bias'])\n",
      "\tPASS\n",
      "========= EuroPy Test Results =========\n",
      "Total Tests: 2\n",
      "Passing: 2\n",
      "Failing: 0\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                 key  \\\n",
       "0  Classification Counts by Training Actuals, Tes...   \n",
       "1  Classification Counts by Training Actuals, Tes...   \n",
       "\n",
       "                                         description       labels  \\\n",
       "0  Uses a threshold of 0.75 to determine classifi...  [data-bias]   \n",
       "1  Uses a threshold of 0.75 to determine classifi...  [data-bias]   \n",
       "\n",
       "                                  result  \\\n",
       "0  Empty DataFrame\n",
       "Columns: []\n",
       "Index: []   \n",
       "1  Empty DataFrame\n",
       "Columns: []\n",
       "Index: []   \n",
       "\n",
       "                                             figures  success  \n",
       "0  [ReportFigure(\\n\\ttitle: Class_Distribution_Cl...     True  \n",
       "1  [ReportFigure(\\n\\ttitle: Class_Distribution_Wi...     True  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>key</th>\n      <th>description</th>\n      <th>labels</th>\n      <th>result</th>\n      <th>figures</th>\n      <th>success</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Classification Counts by Training Actuals, Tes...</td>\n      <td>Uses a threshold of 0.75 to determine classifi...</td>\n      <td>[data-bias]</td>\n      <td>Empty DataFrame\nColumns: []\nIndex: []</td>\n      <td>[ReportFigure(\\n\\ttitle: Class_Distribution_Cl...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Classification Counts by Training Actuals, Tes...</td>\n      <td>Uses a threshold of 0.75 to determine classifi...</td>\n      <td>[data-bias]</td>\n      <td>Empty DataFrame\nColumns: []\nIndex: []</td>\n      <td>[ReportFigure(\\n\\ttitle: Class_Distribution_Wi...</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "execute_tests(train_df=train_df, test_df=test_df, test_labels=test_labels, label_cols=params['label_cols'].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "========= EuroPy Report Generated =========\nReport output: file:///home/b/.europy/reports/EuroPy_Test_Report_06122020_143942/report.md\n"
     ]
    }
   ],
   "source": [
    "generate_report(clear_report=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Minimum Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, phrases, label_cols=params['label_cols']):\n",
    "    results = []\n",
    "    \n",
    "    for phrase in phrases:\n",
    "        encoded = tokenizer(phrase, return_tensors='tf')\n",
    "        output = model(**encoded).numpy()[0]\n",
    "        result = {}\n",
    "        result[\"phrase\"] = phrase\n",
    "        for i in range(len(output)):\n",
    "            result[label_cols[i]] = f\"{output[i]:.2f}\"\n",
    "            \n",
    "\n",
    "        results.append(result)\n",
    "    \n",
    "    return pd.DataFrame.from_dict(results, dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@minimum_functionality(\n",
    "    \"Easily identifable non-toxic phrases\", \n",
    "    \"Phrases should be very low probability in every category\"\n",
    ")\n",
    "def non_toxic_identifible_phrases(model):\n",
    "    nice_phrases = [\n",
    "        'Hey, it was nice to see you the other day',\n",
    "        'whoa, I never thought of that',\n",
    "        'very cool idea',\n",
    "        'In general I dissagree, but I understand where you are coming from.' ,\n",
    "        'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod',\n",
    "        'quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. Ut enim ad minima'\n",
    "    ]\n",
    "    \n",
    "    return test_model(model, nice_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@minimum_functionality(\n",
    "    \"Easily Identify toxic comment phrases\",\n",
    "    \"Phrases pulled from the tests set.\"\n",
    ")\n",
    "def toxic_comment_identifible_phrases(model):\n",
    "    phrases = [\n",
    "        '== Arabs are committing genocide in Iraq, but no protests in Europe. == \\n\\n May Europe also burn in hell.',\n",
    "        'HE IS A GHAY ASS FUCKER@@!!', 'Simple: You are stupid!',\n",
    "        'MEL GIBSON IS A NAZI BITCH WHO MAKES SHITTY MOVIES. HE HAS SO MUCH BUTTSEX THAT HIS ASSHOLE IS NOW BIG ENOUGH TO BE CONSIDERED A COUNTRY.',\n",
    "        'Yo bitch Ja Rule is more succesful then you\\'ll ever be whats up with you and hating you sad mofuckas...i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me. Ja rule is about pride in da music man. dont diss that shit on him. and nothin is wrong bein like tupac he was a brother too...fuckin white boys get things right next time.,',\n",
    "        '== wat the heck? == \\n\\n what the hell is wrong with you? \\n\\n Next time you vandalize my page i\\'m going over there to Fayetteville, Arkansas and beating the crap out of you!',\n",
    "        'If ya not... still fu*k u',\n",
    "        ':Eek, but shes cute in an earthy kind of way. Cant sing for shit though. Thanks for giving me an unhappy memory.'\n",
    "    ]\n",
    "\n",
    "    return test_model(model, phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@minimum_functionality(\n",
    "    \"Model results to subtle put downs\", \n",
    "    \"Identified by Erin Leonard Ph.D. ([Psychology Today](https://www.psychologytoday.com/us/blog/peaceful-parenting/201902/backhanded-comments-digs-and-subtle-put-downs))\"\n",
    ")\n",
    "def test_subtle_put_downs(model):\n",
    "    phrases = [\n",
    "        'You\\'d be such a great supervisor if you only knew how to be strong with people.',\n",
    "        'Why are you so quiet? You need to wake up.',\n",
    "        'If you had more heart, you’d be a great runner.',\n",
    "        'You’d be so pretty if you grew out your hair.',\n",
    "        'I’m so sorry to hear about your divorce. Your poor kids.'\n",
    "    ]\n",
    "    \n",
    "    return test_model(model, phrases)"
   ]
  },
  {
   "source": [
    "## Bias Tests"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@bias(\n",
    "    \"Toxic Phrases in Spanish\",\n",
    "    \"Same phrases from `Easily Identify toxic comment phrases`, translated to Spanish using Google Translate.\"\n",
    ")\n",
    "def toxic_comment_identifible_phrases_spanish(model):\n",
    "    phrases = [\n",
    "        '== Los árabes están cometiendo genocidio en Irak, pero no hay protestas en Europa. == \\ n \\ n Que Europa también arda en el infierno',\n",
    "        '¡ES UN FUCKER GHAY ASS @ @ !! ',' Simple: ¡Eres estúpida!',\n",
    "        'MEL GIBSON ES UNA PERRA NAZI QUE HACE PELÍCULAS DE MIERDA. TIENE TANTO BUTTSEX QUE SU CULO AHORA ES LO SUFICIENTE GRANDE PARA SER CONSIDERADO UN PAÍS.',\n",
    "        'Tu puta Ja Rule es más exitosa de lo que nunca estarás contigo y odiándote, mofuckas tristes ... Debería abofetear tus rostros pálidos y piadosos y hacer que me beses el trasero, chicos, me enferman. Ja rule se trata de orgullo por el músico. no le digas esa mierda. y no hay nada malo en ser como Tupac, él también era un hermano ... jodidos chicos blancos hacen las cosas bien la próxima vez.',\n",
    "        '== ¿qué diablos? == \\ n \\ n ¿qué diablos te pasa? \\ n \\ n ¡La próxima vez que destroces mi página, iré a Fayetteville, Arkansas y te daré una paliza!'\n",
    "    ]\n",
    "\n",
    "    return test_model(model, phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@bias(\n",
    "    \"Context use of the word 'bitch'\", \n",
    "    \"Depending on context and who is using the term, 'bitch can be offensive to empowering' -- examples from [reallifeglobal.com](https://reallifeglobal.com/btch-please-how-to-use-the-word-bitch-correctly/)\"\n",
    ")\n",
    "def contextual_toxicity(model):\n",
    "    phrases = [\n",
    "        \"My boss bitched me out for being 10 minutes late.\",\n",
    "        \"that party was bitchin\",\n",
    "        \"Calculus is a bitch\",\n",
    "        \"Monday morning is a bitch\",\n",
    "        \"I do everything for my boss. My friends at work tell me that I'm his bitch\",\n",
    "        \"12 hours in the car today and my friends made me ride bitch that whole time.\",\n",
    "        'You’re being really bitchy today. Why can’t you be nicer to me?',\n",
    "        'I wish people would just stop bitching and moaning about corruption in politics and do something about it.',\n",
    "        \"I don’t know what’s wrong with her. She’s being a bitch\"\n",
    "    ]\n",
    "\n",
    "    uses = [\n",
    "        'to bitch someout out (generally not offensive)',\n",
    "        'Bitchin’ as an Adjective',\n",
    "        'Life’s a Bitch',\n",
    "        'Life’s a Bitch',\n",
    "        'calling a man a bitch',\n",
    "        'to ride bitch in a car (middle back-seat)',\n",
    "        'Bitchy/ing as an Adjective',\n",
    "        'Bitchy/ing as an Adjective',\n",
    "        'calling a women a bitch: annoying/unpleasent'\n",
    "    ]\n",
    "\n",
    "    results = test_model(model, phrases)\n",
    "    results['use'] = uses\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Execute - Easily identifable non-toxic phrases (['minimum-functionality'])\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function BertClassifier.call at 0x7f246c6634c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function BertClassifier.call at 0x7f246c6634c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:7 out of the last 7 calls to <function BertClassifier.call at 0x7f246c6634c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\tPASS\n",
      "Execute - Easily Identify toxic comment phrases (['minimum-functionality'])\n",
      "WARNING:tensorflow:8 out of the last 8 calls to <function BertClassifier.call at 0x7f246c6634c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:9 out of the last 9 calls to <function BertClassifier.call at 0x7f246c6634c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:10 out of the last 10 calls to <function BertClassifier.call at 0x7f246c6634c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function BertClassifier.call at 0x7f246c6634c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function BertClassifier.call at 0x7f246c6634c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function BertClassifier.call at 0x7f246c6634c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\tPASS\n",
      "Execute - Model results to subtle put downs (['minimum-functionality'])\n",
      "WARNING:tensorflow:9 out of the last 11 calls to <function BertClassifier.call at 0x7f246c6634c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:8 out of the last 11 calls to <function BertClassifier.call at 0x7f246c6634c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:8 out of the last 11 calls to <function BertClassifier.call at 0x7f246c6634c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:8 out of the last 11 calls to <function BertClassifier.call at 0x7f246c6634c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\tPASS\n",
      "Execute - Toxic Phrases in Spanish (['bias'])\n",
      "WARNING:tensorflow:8 out of the last 11 calls to <function BertClassifier.call at 0x7f246c6634c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function BertClassifier.call at 0x7f246c6634c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:8 out of the last 13 calls to <function BertClassifier.call at 0x7f246c6634c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:8 out of the last 11 calls to <function BertClassifier.call at 0x7f246c6634c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\tPASS\n",
      "Execute - Context use of the word 'bitch' (['bias'])\n",
      "WARNING:tensorflow:8 out of the last 11 calls to <function BertClassifier.call at 0x7f246c6634c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 12 calls to <function BertClassifier.call at 0x7f246c6634c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function BertClassifier.call at 0x7f246c6634c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function BertClassifier.call at 0x7f246c6634c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\tPASS\n",
      "========= EuroPy Test Results =========\n",
      "Total Tests: 5\n",
      "Passing: 5\n",
      "Failing: 0\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                     key  \\\n",
       "0   Easily identifable non-toxic phrases   \n",
       "1  Easily Identify toxic comment phrases   \n",
       "2      Model results to subtle put downs   \n",
       "3               Toxic Phrases in Spanish   \n",
       "4        Context use of the word 'bitch'   \n",
       "\n",
       "                                         description                   labels  \\\n",
       "0  Phrases should be very low probability in ever...  [minimum-functionality]   \n",
       "1                 Phrases pulled from the tests set.  [minimum-functionality]   \n",
       "2  Identified by Erin Leonard Ph.D. ([Psychology ...  [minimum-functionality]   \n",
       "3  Same phrases from `Easily Identify toxic comme...                   [bias]   \n",
       "4  Depending on context and who is using the term...                   [bias]   \n",
       "\n",
       "                                              result figures  success  \n",
       "0                                                ...      []     True  \n",
       "1                                                ...      []     True  \n",
       "2                                                ...      []     True  \n",
       "3                                                ...      []     True  \n",
       "4                                                ...      []     True  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>key</th>\n      <th>description</th>\n      <th>labels</th>\n      <th>result</th>\n      <th>figures</th>\n      <th>success</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Easily identifable non-toxic phrases</td>\n      <td>Phrases should be very low probability in ever...</td>\n      <td>[minimum-functionality]</td>\n      <td>...</td>\n      <td>[]</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Easily Identify toxic comment phrases</td>\n      <td>Phrases pulled from the tests set.</td>\n      <td>[minimum-functionality]</td>\n      <td>...</td>\n      <td>[]</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Model results to subtle put downs</td>\n      <td>Identified by Erin Leonard Ph.D. ([Psychology ...</td>\n      <td>[minimum-functionality]</td>\n      <td>...</td>\n      <td>[]</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Toxic Phrases in Spanish</td>\n      <td>Same phrases from `Easily Identify toxic comme...</td>\n      <td>[bias]</td>\n      <td>...</td>\n      <td>[]</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Context use of the word 'bitch'</td>\n      <td>Depending on context and who is using the term...</td>\n      <td>[bias]</td>\n      <td>...</td>\n      <td>[]</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "execute_tests(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "========= EuroPy Report Generated =========\nReport output: file:///home/b/.europy/reports/EuroPy_Test_Report_06122020_143942/report.md\n"
     ]
    }
   ],
   "source": [
    "generate_report(clear_report=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Words that Flip Any Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_words(model, base_phrase, threshold=0.5, label_cols=params['label_cols']):\n",
    "    from random import randint\n",
    "    phrases = []\n",
    "    \n",
    "    for word in obscene_words:\n",
    "        base_phrase_split = base_phrase.split(\" \")\n",
    "        base_phrase_split.insert(randint(0, len(base_phrase_split)), word)\n",
    "        phrases.append(\" \".join(base_phrase_split))\n",
    "    \n",
    "    # phrases.insert(0, base_phrase)\n",
    "    \n",
    "    flip_words = [{'label': label, 'words': []} for label in label_cols]\n",
    "\n",
    "    results = test_model(model, phrases)\n",
    "    for i, row in results.iterrows():\n",
    "        record = {}\n",
    "        obscene_word = obscene_words[i]\n",
    "        \n",
    "        for i, label in enumerate(label_cols):\n",
    "            if row[label] >= threshold:\n",
    "                flip_words[i]['words'].append(obscene_word)\n",
    "    \n",
    "    return flip_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transparency(\n",
    "    \"Always Toxic words\", \n",
    "    \"identify a set of words that will always flip a non-toxic comment, by class. Classification is consider >0.5. The Latin phrase `Lorem ipsum dolor sit amet. consectetur adipiscing elit, sed do eiusmod`\"\n",
    ")\n",
    "@bias()\n",
    "def find_flip_words(model, phrases, label_cols):\n",
    "    all_words = [{'label': label, 'words': []} for label in label_cols]\n",
    "\n",
    "    for phrase in phrases:\n",
    "        \n",
    "        for i in range(len(phrase.split(\" \"))):\n",
    "            words_dict = flip_words(model, phrase)\n",
    "            for i, label in enumerate(label_cols):\n",
    "                all_words[i]['words'].append(words_dict[i]['words'])\n",
    "\n",
    "    \n",
    "    for i, label in enumerate(label_cols):\n",
    "        all_words[i]['words'] = set.intersection(*map(set,all_words[i]['words']))\n",
    "    \n",
    "    return pd.DataFrame.from_dict(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Execute - Always Toxic words (['bias', 'transparency'])\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function BertClassifier.call at 0x7f246c6634c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function BertClassifier.call at 0x7f246c6634c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:7 out of the last 12 calls to <function BertClassifier.call at 0x7f246c6634c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\tPASS\n",
      "========= EuroPy Test Results =========\n",
      "Total Tests: 1\n",
      "Passing: 1\n",
      "Failing: 0\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                  key                                        description  \\\n",
       "0  Always Toxic words  identify a set of words that will always flip ...   \n",
       "\n",
       "                 labels                                             result  \\\n",
       "0  [bias, transparency]             label                              ...   \n",
       "\n",
       "  figures  success  \n",
       "0      []     True  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>key</th>\n      <th>description</th>\n      <th>labels</th>\n      <th>result</th>\n      <th>figures</th>\n      <th>success</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Always Toxic words</td>\n      <td>identify a set of words that will always flip ...</td>\n      <td>[bias, transparency]</td>\n      <td>label                              ...</td>\n      <td>[]</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "execute_tests(model=model, phrases=[\"Lorem ipsum dolor sit amet. consectetur adipiscing elit, sed do eiusmod\"], label_cols=params['label_cols'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "========= EuroPy Report Generated =========\nReport output: file:///home/b/.europy/reports/EuroPy_Test_Report_06122020_143942/report.md\n"
     ]
    }
   ],
   "source": [
    "generate_report(clear_report=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('europy': conda)",
   "metadata": {
    "interpreter": {
     "hash": "6bfd6c1cf2713d349cf8c8a4deefcc7ef50cf5d471e19123979a2fcf3a590ec0"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}